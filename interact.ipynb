{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT_FILE = 'byui-python-analysis-30a31cf00f2c.json'\n",
    "REPORT_REQUEST_FILE = 'report-request.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "import demjson\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import json\n",
    "import shutil\n",
    "import base64\n",
    "import hashlib\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_FILE = './cache/{hash}/columns.csv'\n",
    "REQUEST_FILE = './cache/{hash}/{date}/info.csv'\n",
    "SUMMARY_FILE = './cache/{hash}/{date}/summary.csv'\n",
    "DATA_FILE = './cache/{hash}/{date}/pages/{page}.csv'\n",
    "CACHE_FILE = './cache/raw/{hash}_{date}_{page}.json'\n",
    "GA_DATE_FORMAT = '%Y-%m-%d'\n",
    "\n",
    "def report_to_frame(report):\n",
    "    # Get metric names as list\n",
    "    metric_names = [metric.get('name') for metric in report.get('columnHeader').get('metricHeader').get('metricHeaderEntries',[])]    \n",
    "    metric_names = [re.sub(r'^ga:','', name) for name in metric_names]\n",
    "    \n",
    "    # Get dimension names as list\n",
    "    dimension_names = report.get('columnHeader').get('dimensions',[])\n",
    "    dimension_names = [re.sub(r'^ga:','', name) for name in dimension_names]\n",
    "    \n",
    "    # Collect dimension values into a dataframe\n",
    "    dimensions = [row.get('dimensions',[]) for row in report.get('data').get('rows',[])]\n",
    "    dimensions = pd.DataFrame(dimensions, columns = dimension_names)\n",
    "    \n",
    "    # Create MultiIndex from dimension values\n",
    "    index = pd.MultiIndex.from_frame(dimensions) if not dimensions.empty else None\n",
    "    \n",
    "    # Collect metric values into list of lists\n",
    "    rows = []\n",
    "    num_date_ranges = 1\n",
    "    for row in report.get('data').get('rows',[]):\n",
    "        # Concat all metrics for each date range, will seperate later\n",
    "        values = []\n",
    "        num_date_ranges = len(row.get('metrics'))\n",
    "        for date_range in row.get('metrics'):\n",
    "            values += date_range.get('values')\n",
    "        rows.append(values)\n",
    "    \n",
    "    \n",
    "    columns = None\n",
    "    if num_date_ranges > 1:\n",
    "        # Create column index as (date_ranges x metrics)\n",
    "        columns = pd.MultiIndex.from_product([range(num_date_ranges), metric_names], names=('date_range','metric'))\n",
    "    else:\n",
    "        columns = pd.Index(metric_names)\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(rows, index=index, columns=columns)\n",
    "\n",
    "    # Convert to numbers\n",
    "    df = df.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "    return df\n",
    "\n",
    "def report_summary(report):\n",
    "    metric_names = [re.sub(r'^ga:','', metric.get('name')) for metric in report.get('columnHeader').get('metricHeader').get('metricHeaderEntries')]\n",
    "    num_date_ranges = len(report.get('data').get('totals'))\n",
    "    data = {'min':[],'max':[],'sum':[]}\n",
    "    total_rows = int(report.get('data').get('rowCount',0))\n",
    "    \n",
    "    int(report.get('data').get('rowCount',0))\n",
    "    \n",
    "    for sums in report.get('data').get('totals'):\n",
    "        data['sum'] += sums.get('values')\n",
    "    for sums in report.get('data').get('minimums'):\n",
    "        data['min'] += sums.get('values')\n",
    "    for sums in report.get('data').get('maximums'):\n",
    "        data['max'] += sums.get('values')\n",
    "        \n",
    "    index = None\n",
    "    if num_date_ranges > 1:\n",
    "        index = pd.MultiIndex.from_product([range(num_date_ranges), metric_names], names=('date_range','metric'))\n",
    "    else:\n",
    "        index = pd.Index(metric_names)\n",
    "    \n",
    "    df = pd.DataFrame(data, index=index)\n",
    "    df = df.apply(pd.to_numeric, errors='ignore')\n",
    "    df['avg'] = df['sum'] / total_rows\n",
    "#     df = df.transpose()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def report_columns(report):\n",
    "    dimensions = report.get('columnHeader').get('dimensions',[])\n",
    "    dimensions = pd.DataFrame(dimensions, columns=['name'])\n",
    "    dimensions['type'] = 'dimension'\n",
    "    metrics = report.get('columnHeader').get('metricHeader').get('metricHeaderEntries',[])\n",
    "    metrics = pd.DataFrame(metrics)\n",
    "    metrics = metrics.rename(columns={'type':'format'})\n",
    "    metrics['type'] = 'metric'\n",
    "    return pd.concat([dimensions, metrics], sort=False)\n",
    "\n",
    "def report_info(report):\n",
    "    data = report.get('data')\n",
    "    nrows = int(data.get('rowCount',0))\n",
    "    reads = data.get('samplesReadCounts',[nrows])[0]\n",
    "    space = data.get('samplingSpaceSizes',[nrows])[0]\n",
    "    return pd.Series({\n",
    "        'nrows': nrows,\n",
    "        'reads': reads,\n",
    "        'space': space,\n",
    "        'rate': reads/space,\n",
    "        'golden': data.get('isDataGolden',False),\n",
    "        'date': data.get('dataLastRefreshed', datetime.now()),\n",
    "    }).to_frame(name='values')\n",
    "\n",
    "def hash_dict(d):\n",
    "    string = json.dumps(d)\n",
    "    buffer = str.encode(string)\n",
    "    hashed = hashlib.sha1(buffer).digest()\n",
    "    encoded = base64.b64encode(hashed).decode('utf-8')\n",
    "    clean = re.sub(r'\\W','',encoded)\n",
    "    return clean[0:10]\n",
    "\n",
    "def each_day(date_ranges):\n",
    "    for date_range in date_ranges:\n",
    "        start_date = datetime.strptime(date_range.get('startDate'), GA_DATE_FORMAT)\n",
    "        end_date = datetime.strptime(date_range.get('endDate'), GA_DATE_FORMAT)\n",
    "        for n in range(int ((end_date - start_date).days)):\n",
    "            yield (start_date + timedelta(days=n)).strftime(GA_DATE_FORMAT)\n",
    "\n",
    "def each_request(requests):\n",
    "    for r in requests:\n",
    "        request = r.copy()\n",
    "        \n",
    "        # Set page size to default to max, so that making less requests\n",
    "        request['pageSize'] = request.get('pageSize',100000)\n",
    "\n",
    "        # Set sample size to default to large\n",
    "        request['samplingLevel'] = request.get('samplingLevel','LARGE')\n",
    "\n",
    "        # Remove date ranges and page token\n",
    "        date_ranges = request.pop('dateRanges',[])\n",
    "        page_token = request.pop('pageToken',0)\n",
    "        \n",
    "        hashed = hash_dict(request)\n",
    "\n",
    "        for date in each_day(date_ranges):\n",
    "            r = {\n",
    "                'hash': hashed,\n",
    "                'date': date,            \n",
    "                'page': page_token,\n",
    "                'body': request,\n",
    "            }\n",
    "            Path(os.path.dirname(DATA_FILE.format(**r))).mkdir(parents=True, exist_ok=True)\n",
    "            Path(os.path.dirname(CACHE_FILE.format(**r))).mkdir(parents=True, exist_ok=True)\n",
    "            yield r\n",
    "            \n",
    "\n",
    "def get_report(request):\n",
    "    file = CACHE_FILE.format(**request)\n",
    "    \n",
    "    # Check to see if file is in cache\n",
    "    if os.path.exists(file):\n",
    "        with open(file) as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    # Modify request Body\n",
    "    body = request['body'].copy()\n",
    "    body['pageToken'] = str(request['page'])\n",
    "    body['dateRanges'] = [{\n",
    "        'startDate':request['date'],\n",
    "        'endDate':request['date']\n",
    "    }]\n",
    "    \n",
    "    # Make the request\n",
    "    print('{hash}\\t{date}\\t{page}'.format(**request))\n",
    "    r = analytics.reports().batchGet(body={'reportRequests':[body]}).execute()\n",
    "    report = r.get('reports')[0]\n",
    "    \n",
    "    with open(file,'w') as f:\n",
    "        json.dump(report, f)\n",
    "    \n",
    "    return report\n",
    "\n",
    "def write_csvs(request, report):\n",
    "    if not os.path.exists(COLUMNS_FILE.format(**request)):\n",
    "        columns = report_columns(report)\n",
    "        columns.index.name = 'row'\n",
    "        columns = columns.reset_index()\n",
    "        columns['request'] = request['hash']\n",
    "        columns = columns.set_index(['request','row'])\n",
    "        columns.to_csv(COLUMNS_FILE.format(**request))\n",
    "    \n",
    "    if not os.path.exists(REQUEST_FILE.format(**request)):\n",
    "        info = report_info(report)\n",
    "        info['request'] = request['hash']\n",
    "        info['date'] = request['date']\n",
    "        info = info.set_index(['request','date'])\n",
    "        info.to_csv(REQUEST_FILE.format(**request))\n",
    "    \n",
    "    if not os.path.exists(SUMMARY_FILE.format(**request)):\n",
    "        summary = report_summary(report)\n",
    "        summary.index.name = 'metric'\n",
    "        summary = summary.reset_index()\n",
    "        summary['request'] = request['hash']\n",
    "        summary['date'] = request['date']\n",
    "        summary = summary.set_index(['request','date','metric'])\n",
    "        summary.to_csv(SUMMARY_FILE.format(**request))\n",
    "\n",
    "    if not os.path.exists(DATA_FILE.format(**request)):\n",
    "        report_to_frame(report).to_csv(DATA_FILE.format(**request), header=False)\n",
    "\n",
    "def concat_files(sources, destination):\n",
    "    with open(destination,'ab') as wfd:\n",
    "        for f in sources:\n",
    "            with open(f,'rb') as fd:\n",
    "                shutil.copyfileobj(fd, wfd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Initalize API with Service Account File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=['https://www.googleapis.com/auth/analytics.readonly'])\n",
    "analytics = build('analyticsreporting', 'v4', credentials=credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Request All Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rids = set()\n",
    "with open(REPORT_REQUEST_FILE) as f:\n",
    "    requests = demjson.decode(f.read())\n",
    "    \n",
    "    for request in each_request(requests):\n",
    "        \n",
    "        rids.add(request['hash'])\n",
    "        report = get_report(request)\n",
    "        write_csvs(request, report)\n",
    "        \n",
    "        while 'nextPageToken' in report:\n",
    "            request['page'] = report['nextPageToken']\n",
    "            report = get_report(request)\n",
    "            write_csvs(request, report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Concat Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time = datetime.now().strftime(\"%h%d_%H%M\")\n",
    "for rid in rids:\n",
    "    file = './{}_{}.csv'.format(run_time, rid)\n",
    "    columns = pd.read_csv('./cache/{}/columns.csv'.format(rid))\n",
    "    pd.DataFrame(columns=columns['name']).to_csv(file, index=False)\n",
    "    concat_files(glob('./cache/{}/*/pages/*.csv'.format(rid)), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
